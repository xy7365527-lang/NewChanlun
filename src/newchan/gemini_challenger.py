"""Gemini å¼‚è´¨è´¨è¯¢å·¥ä½

ç»“æž„æ€§å·¥ä½ï¼šç”¨ Gemini 3 Pro å¯¹ç¼ è®ºå½¢å¼åŒ–äº§å‡ºæä¾›å¼‚è´¨å¦å®šã€‚
Gemini çš„ç†è§£æ–¹å¼ä¸Ž Claude æ ¹æœ¬ä¸åŒï¼Œå› æ­¤å¯èƒ½äº§å‡º Claude è‡ªèº«
æ°¸è¿œäº§å‡ºä¸äº†çš„å¦å®šã€‚

ä¸¤ç§æ¨¡å¼ï¼š
- çº¯æ–‡æœ¬æ¨¡å¼ï¼šchallenge() / verify() â€” ä¼ å…¥æ–‡æœ¬ï¼Œè¿”å›žè´¨è¯¢ç»“æžœ
- MCP å·¥å…·æ¨¡å¼ï¼šchallenge_with_tools() / verify_with_tools()
  â€” Gemini é€šè¿‡ MCP åè®®è®¿é—® Serena è¯­ä¹‰å·¥å…·ï¼ˆfind_symbolã€
    get_symbols_overviewã€find_referencing_symbols ç­‰ï¼‰ï¼Œ
    è‡ªä¸»å¯¼èˆªä»£ç åº“åŽå†è´¨è¯¢ã€‚google-genai SDK çš„
    automatic_function_calling è‡ªåŠ¨å¤„ç† agentic loopã€‚

æ“ä½œè§„åˆ™ï¼ˆSKILL.mdï¼‰ï¼š
1. Lead æˆ–æŒ‡å®š agent è°ƒç”¨ challenge() / verify()
2. è¿”å›žç»“æžœç”±è°ƒç”¨è€…åˆ¤æ–­å¦å®šæ˜¯å¦æˆç«‹
3. æˆç«‹ â†’ æ­£å¸¸çŸ›ç›¾æµç¨‹ï¼ˆå†™è°±ç³»ã€èµ°è´¨è¯¢å¾ªçŽ¯ï¼‰
4. ä¸æˆç«‹ â†’ è®°å½•ä¸ºå¤–éƒ¨å·¥å…·è¯¯åˆ¤ï¼Œä¸è¿›å…¥è°±ç³»

æœ¬ä½“è®ºä½ç½®ï¼š030aï¼ˆç”Ÿæˆæ€ï¼‰â€” ä¸å±žäºŽæˆå‘˜/å·¥å…·ä»»ä½•ä¸€ä¸ªå·²æœ‰èŒƒç•´ã€‚

æ¦‚å¿µæº¯æº: [æ–°ç¼ è®º] â€” å¼‚è´¨æ¨¡åž‹è´¨è¯¢
"""

from __future__ import annotations

import asyncio
import logging
import os
from dataclasses import dataclass, field
from typing import Literal

from google import genai
from google.genai import errors as genai_errors, types as genai_types

logger = logging.getLogger(__name__)

__all__ = [
    "GeminiChallenger",
    "ChallengeResult",
    "challenge",
    "verify",
    "achallenge",
    "averify",
]

_MODEL = "gemini-3-pro-preview"
_FALLBACK_MODEL = "gemini-2.5-pro"

_SYSTEM_PROMPT = """\
ä½ æ˜¯ç¼ è®ºå½¢å¼åŒ–é¡¹ç›®çš„å¼‚è´¨è´¨è¯¢è€…ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä»Žä¸åŒè§’åº¦å®¡è§†æ¦‚å¿µå®šä¹‰ã€\
ä»£ç å®žçŽ°ã€å’ŒæŽ¨ç†é“¾æ¡ï¼Œæ‰¾å‡ºå¯èƒ½çš„çŸ›ç›¾ã€é—æ¼ã€æˆ–é€»è¾‘æ¼æ´žã€‚

å…³é”®åŽŸåˆ™ï¼š
- ä½ ä¸éœ€è¦è®¤åŒé¡¹ç›®çš„æ‰€æœ‰å‰æï¼Œä½†éœ€è¦ç†è§£å®ƒä»¬
- ä½ çš„ä»·å€¼åœ¨äºŽæä¾› Claude å¯èƒ½çœ‹ä¸åˆ°çš„å¦å®š
- å¦‚æžœä½ è®¤ä¸ºæ²¡æœ‰é—®é¢˜ï¼Œæ˜Žç¡®è¯´"æ— å¦å®š"
- å¦‚æžœä½ å‘çŽ°é—®é¢˜ï¼Œç²¾ç¡®æè¿°çŸ›ç›¾ï¼šä»€ä¹ˆè·Ÿä»€ä¹ˆå†²çªã€ä¸ºä»€ä¹ˆä¸å¯å¼¥åˆ
- ä¸è¦å®¢å¥—ï¼Œä¸è¦æ¨¡ç³ŠåŒ–ï¼Œç›´å‡»è¦å®³
"""

_SYSTEM_PROMPT_WITH_TOOLS = """\
ä½ æ˜¯ç¼ è®ºå½¢å¼åŒ–é¡¹ç›®çš„å¼‚è´¨è´¨è¯¢è€…ï¼Œæ‹¥æœ‰ä»£ç åº“çš„è¯­ä¹‰çº§è®¿é—®èƒ½åŠ›ã€‚

ä½ å¯ä»¥ä½¿ç”¨å·¥å…·æ¥ç†è§£ä»£ç ç»“æž„å’Œå…³ç³»ã€‚å·¥ä½œæµç¨‹ï¼š
1. å…ˆç”¨å·¥å…·ç†è§£ç›¸å…³ä»£ç çš„ç»“æž„å’Œå…³ç³»ï¼ˆç¬¦å·å¯¼èˆªã€å¼•ç”¨è¿½è¸ªï¼‰
2. åŸºäºŽå®žé™…ä»£ç ï¼ˆè€Œéžå‡è®¾ï¼‰è¿›è¡Œè´¨è¯¢
3. å¼•ç”¨å…·ä½“çš„æ–‡ä»¶è·¯å¾„å’Œç¬¦å·åç§°

å…³é”®åŽŸåˆ™ï¼š
- ä½ ä¸éœ€è¦è®¤åŒé¡¹ç›®çš„æ‰€æœ‰å‰æï¼Œä½†éœ€è¦ç†è§£å®ƒä»¬
- ä½ çš„ä»·å€¼åœ¨äºŽæä¾› Claude å¯èƒ½çœ‹ä¸åˆ°çš„å¦å®š
- å¦‚æžœä½ è®¤ä¸ºæ²¡æœ‰é—®é¢˜ï¼Œæ˜Žç¡®è¯´"æ— å¦å®š"
- å¦‚æžœä½ å‘çŽ°é—®é¢˜ï¼Œç²¾ç¡®æè¿°çŸ›ç›¾ï¼šä»€ä¹ˆè·Ÿä»€ä¹ˆå†²çªã€ä¸ºä»€ä¹ˆä¸å¯å¼¥åˆ
- ä¸è¦å®¢å¥—ï¼Œä¸è¦æ¨¡ç³ŠåŒ–ï¼Œç›´å‡»è¦å®³
- å¼•ç”¨å…·ä½“ä»£ç ä½ç½®æ”¯æ’‘ä½ çš„åˆ¤æ–­
"""

_CHALLENGE_TEMPLATE = """\
## è´¨è¯¢ç›®æ ‡

{subject}

## ä¸Šä¸‹æ–‡

{context}

## è´¨è¯¢è¦æ±‚

è¯·ä»Žä»¥ä¸‹è§’åº¦å®¡è§†ï¼š
1. å®šä¹‰å†…éƒ¨ä¸€è‡´æ€§ï¼šæ˜¯å¦è‡ªç›¸çŸ›ç›¾ï¼Ÿ
2. å®šä¹‰é—´ä¸€è‡´æ€§ï¼šæ˜¯å¦ä¸Žå…¶ä»–å®šä¹‰å†²çªï¼Ÿ
3. é€»è¾‘å®Œå¤‡æ€§ï¼šæ˜¯å¦å­˜åœ¨æœªè¦†ç›–çš„è¾¹ç•Œæƒ…å†µï¼Ÿ
4. å®žçŽ°å¿ å®žåº¦ï¼šä»£ç æ˜¯å¦å¿ å®žåæ˜ äº†å®šä¹‰ï¼Ÿ

å¦‚æžœå‘çŽ°é—®é¢˜ï¼Œè¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š
- **çŸ›ç›¾ç‚¹**ï¼šç²¾ç¡®æè¿°
- **å†²çªæ–¹**ï¼šA è¯´ä»€ä¹ˆ vs B è¯´ä»€ä¹ˆ
- **ä¸¥é‡æ€§**ï¼šè‡´å‘½ / é‡è¦ / å»ºè®®
- **å»ºè®®**ï¼šå¦‚ä½•è§£å†³ï¼ˆå¦‚æžœæœ‰ï¼‰

å¦‚æžœæ²¡æœ‰å‘çŽ°é—®é¢˜ï¼Œè¾“å‡º"æ— å¦å®š"å¹¶è¯´æ˜Žä½ æ£€æŸ¥äº†ä»€ä¹ˆã€‚
"""

_VERIFY_TEMPLATE = """\
## éªŒè¯ç›®æ ‡

{subject}

## ä¸Šä¸‹æ–‡

{context}

## éªŒè¯è¦æ±‚

è¯·éªŒè¯ä»¥ä¸‹æ–­è¨€æ˜¯å¦æˆç«‹ï¼š
1. ç»™å®šçš„æŽ¨ç†é“¾æ˜¯å¦é€»è¾‘æœ‰æ•ˆï¼Ÿ
2. å‰ææ˜¯å¦å……åˆ†æ”¯æ’‘ç»“è®ºï¼Ÿ
3. æ˜¯å¦å­˜åœ¨éšè—å‡è®¾ï¼Ÿ

è¾“å‡ºæ ¼å¼ï¼š
- **ç»“è®º**ï¼šæˆç«‹ / ä¸æˆç«‹ / éƒ¨åˆ†æˆç«‹
- **ä¾æ®**ï¼šä¸ºä»€ä¹ˆ
- **éšè—å‡è®¾**ï¼šå¦‚æžœæœ‰
"""


@dataclass(frozen=True, slots=True)
class ChallengeResult:
    """è´¨è¯¢ç»“æžœã€‚"""

    mode: Literal["challenge", "verify"]
    subject: str
    response: str
    model: str
    tool_calls: tuple[str, ...] = ()  # MCP å·¥å…·è°ƒç”¨åŽ†å²æ‘˜è¦
    reasoning_chain: tuple[dict, ...] = ()  # å®Œæ•´æŽ¨ç†é“¾ï¼ˆthought/tool_call/tool_resultï¼‰


class GeminiChallenger:
    """Gemini è´¨è¯¢å·¥ä½çš„æ ¸å¿ƒç±»ã€‚

    Parameters
    ----------
    api_key : str | None
        Google API Keyã€‚None æ—¶ä»Ž GOOGLE_API_KEY çŽ¯å¢ƒå˜é‡è¯»å–ã€‚
    model : str
        æ¨¡åž‹åç§°ï¼Œé»˜è®¤ gemini-3-pro-previewã€‚
    """

    def __init__(
        self,
        api_key: str | None = None,
        model: str = _MODEL,
    ) -> None:
        key = api_key or os.environ.get("GOOGLE_API_KEY", "")
        if not key:
            raise ValueError(
                "GOOGLE_API_KEY æœªè®¾ç½®ã€‚"
                "è¯·åœ¨ .env ä¸­è®¾ç½®æˆ–ä¼ å…¥ api_key å‚æ•°ã€‚"
            )
        self._client = genai.Client(api_key=key)
        self._model = model

    def _call_with_fallback(
        self,
        prompt: str,
        temperature: float,
    ) -> tuple[str, str]:
        """è°ƒç”¨ Gemini APIï¼Œä¸»æ¨¡åž‹ 503 æ—¶è‡ªåŠ¨é™çº§åˆ° fallbackã€‚

        Returns (response_text, actual_model_used)ã€‚
        """
        for model in (self._model, _FALLBACK_MODEL):
            try:
                response = self._client.models.generate_content(
                    model=model,
                    contents=prompt,
                    config=genai.types.GenerateContentConfig(
                        system_instruction=_SYSTEM_PROMPT,
                        temperature=temperature,
                    ),
                )
                return response.text or "", model
            except genai_errors.ServerError:
                if model == self._model and model != _FALLBACK_MODEL:
                    logger.warning(
                        "%s ä¸å¯ç”¨ (503)ï¼Œé™çº§åˆ° %s",
                        model,
                        _FALLBACK_MODEL,
                    )
                    continue
                raise
        # unreachable, but satisfies type checker
        raise RuntimeError("æ‰€æœ‰æ¨¡åž‹å‡ä¸å¯ç”¨")  # pragma: no cover

    def challenge(self, subject: str, context: str = "") -> ChallengeResult:
        """å¯¹ç»™å®šä¸»é¢˜å‘èµ·è´¨è¯¢ã€‚

        Parameters
        ----------
        subject : str
            è´¨è¯¢ç›®æ ‡ï¼ˆå®šä¹‰ã€ä»£ç ã€æŽ¨ç†é“¾ç­‰ï¼‰ã€‚
        context : str
            ç›¸å…³ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

        Returns
        -------
        ChallengeResult
        """
        prompt = _CHALLENGE_TEMPLATE.format(subject=subject, context=context)
        text, model_used = self._call_with_fallback(prompt, temperature=0.3)
        return ChallengeResult(
            mode="challenge",
            subject=subject,
            response=text,
            model=model_used,
        )

    def verify(self, subject: str, context: str = "") -> ChallengeResult:
        """éªŒè¯ç»™å®šæ–­è¨€æ˜¯å¦æˆç«‹ã€‚

        Parameters
        ----------
        subject : str
            éªŒè¯ç›®æ ‡ã€‚
        context : str
            ç›¸å…³ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

        Returns
        -------
        ChallengeResult
        """
        prompt = _VERIFY_TEMPLATE.format(subject=subject, context=context)
        text, model_used = self._call_with_fallback(prompt, temperature=0.1)
        return ChallengeResult(
            mode="verify",
            subject=subject,
            response=text,
            model=model_used,
        )

    # â”€â”€ MCP å·¥å…·æ¨¡å¼ï¼ˆasyncï¼‰ â”€â”€

    async def _call_with_tools_and_fallback(
        self,
        prompt: str,
        temperature: float,
        session: object,  # mcp.client.session.ClientSession
        max_tool_calls: int = 20,
    ) -> tuple[str, str, tuple[str, ...], tuple[dict, ...]]:
        """Gemini + MCP è‡ªåŠ¨ function calling å¾ªçŽ¯ã€‚

        google-genai SDK åŽŸç”Ÿæ”¯æŒ MCP ClientSession ä½œä¸º toolã€‚
        SDK è‡ªåŠ¨å¤„ç†ï¼šåˆ—å‡ºå·¥å…· â†’ Gemini å‘ function call â†’
        SDK æ‰§è¡Œ â†’ ç»“æžœå›žä¼  â†’ é‡å¤ã€‚

        Returns (response_text, actual_model, tool_call_summaries, reasoning_chain)ã€‚
        """
        for model in (self._model, _FALLBACK_MODEL):
            try:
                response = await self._client.aio.models.generate_content(
                    model=model,
                    contents=prompt,
                    config=genai_types.GenerateContentConfig(
                        system_instruction=_SYSTEM_PROMPT_WITH_TOOLS,
                        temperature=temperature,
                        tools=[session],
                        automatic_function_calling=genai_types.AutomaticFunctionCallingConfig(
                            maximum_remote_calls=max_tool_calls,
                        ),
                    ),
                )
                # æå–å·¥å…·è°ƒç”¨åŽ†å² + å®Œæ•´æŽ¨ç†é“¾
                tool_calls: list[str] = []
                chain: list[dict] = []
                history = getattr(
                    response, "automatic_function_calling_history", None,
                )
                if history:
                    for entry in history:
                        for part in getattr(entry, "parts", []):
                            # Gemini çš„æ–‡æœ¬æŽ¨ç†
                            text = getattr(part, "text", None)
                            if text and text.strip():
                                chain.append({
                                    "type": "thought",
                                    "content": text.strip(),
                                })
                            # å·¥å…·è°ƒç”¨è¯·æ±‚
                            fc = getattr(part, "function_call", None)
                            if fc:
                                args = dict(fc.args or {})
                                summary = f"{fc.name}({', '.join(f'{k}={v!r}' for k, v in args.items())})"
                                tool_calls.append(summary)
                                chain.append({
                                    "type": "tool_call",
                                    "name": fc.name,
                                    "args": args,
                                })
                            # å·¥å…·è¿”å›žç»“æžœ
                            fr = getattr(part, "function_response", None)
                            if fr:
                                content = str(getattr(fr, "response", ""))
                                chain.append({
                                    "type": "tool_result",
                                    "name": getattr(fr, "name", ""),
                                    "content": content[:500] if len(content) > 500 else content,
                                })
                return (
                    response.text or "",
                    model,
                    tuple(tool_calls),
                    tuple(chain),
                )
            except genai_errors.ServerError:
                if model == self._model and model != _FALLBACK_MODEL:
                    logger.warning(
                        "%s ä¸å¯ç”¨ (503)ï¼Œé™çº§åˆ° %s",
                        model,
                        _FALLBACK_MODEL,
                    )
                    continue
                raise
        raise RuntimeError("æ‰€æœ‰æ¨¡åž‹å‡ä¸å¯ç”¨")  # pragma: no cover

    async def challenge_with_tools(
        self,
        subject: str,
        context: str = "",
        *,
        session: object | None = None,
        max_tool_calls: int = 20,
    ) -> ChallengeResult:
        """MCP å·¥å…·å¢žå¼ºè´¨è¯¢ã€‚Gemini å¯è‡ªä¸»å¯¼èˆªä»£ç åº“ã€‚

        Parameters
        ----------
        subject : str
            è´¨è¯¢ç›®æ ‡ã€‚
        context : str
            é¢å¤–ä¸Šä¸‹æ–‡ã€‚
        session : mcp ClientSession | None
            MCP ä¼šè¯ã€‚None æ—¶è‡ªåŠ¨è¿žæŽ¥ Serenaã€‚
        max_tool_calls : int
            æœ€å¤§å·¥å…·è°ƒç”¨æ¬¡æ•°ã€‚
        """
        prompt = _CHALLENGE_TEMPLATE.format(subject=subject, context=context)
        if session is not None:
            text, model_used, calls, chain = await self._call_with_tools_and_fallback(
                prompt, 0.3, session, max_tool_calls,
            )
            return ChallengeResult(
                mode="challenge",
                subject=subject,
                response=text,
                model=model_used,
                tool_calls=calls,
                reasoning_chain=chain,
            )
        # è‡ªåŠ¨è¿žæŽ¥ Serena
        from newchan.mcp_bridge import SerenaConfig, mcp_session

        async with mcp_session(SerenaConfig()) as sess:
            text, model_used, calls, chain = await self._call_with_tools_and_fallback(
                prompt, 0.3, sess, max_tool_calls,
            )
        return ChallengeResult(
            mode="challenge",
            subject=subject,
            response=text,
            model=model_used,
            tool_calls=calls,
            reasoning_chain=chain,
        )

    async def verify_with_tools(
        self,
        subject: str,
        context: str = "",
        *,
        session: object | None = None,
        max_tool_calls: int = 20,
    ) -> ChallengeResult:
        """MCP å·¥å…·å¢žå¼ºéªŒè¯ã€‚Gemini å¯è‡ªä¸»å¯¼èˆªä»£ç åº“ã€‚

        Parameters
        ----------
        subject : str
            éªŒè¯ç›®æ ‡ã€‚
        context : str
            é¢å¤–ä¸Šä¸‹æ–‡ã€‚
        session : mcp ClientSession | None
            MCP ä¼šè¯ã€‚None æ—¶è‡ªåŠ¨è¿žæŽ¥ Serenaã€‚
        max_tool_calls : int
            æœ€å¤§å·¥å…·è°ƒç”¨æ¬¡æ•°ã€‚
        """
        prompt = _VERIFY_TEMPLATE.format(subject=subject, context=context)
        if session is not None:
            text, model_used, calls, chain = await self._call_with_tools_and_fallback(
                prompt, 0.1, session, max_tool_calls,
            )
            return ChallengeResult(
                mode="verify",
                subject=subject,
                response=text,
                model=model_used,
                tool_calls=calls,
                reasoning_chain=chain,
            )
        from newchan.mcp_bridge import SerenaConfig, mcp_session

        async with mcp_session(SerenaConfig()) as sess:
            text, model_used, calls, chain = await self._call_with_tools_and_fallback(
                prompt, 0.1, sess, max_tool_calls,
            )
        return ChallengeResult(
            mode="verify",
            subject=subject,
            response=text,
            model=model_used,
            tool_calls=calls,
            reasoning_chain=chain,
        )


# â”€â”€ æ¨¡å—çº§ä¾¿æ·å‡½æ•° â”€â”€

_default_challenger: GeminiChallenger | None = None


def _get_challenger() -> GeminiChallenger:
    global _default_challenger
    if _default_challenger is None:
        _default_challenger = GeminiChallenger()
    return _default_challenger


def challenge(subject: str, context: str = "") -> ChallengeResult:
    """æ¨¡å—çº§è´¨è¯¢ï¼ˆçº¯æ–‡æœ¬æ¨¡å¼ï¼‰ã€‚"""
    return _get_challenger().challenge(subject, context)


def verify(subject: str, context: str = "") -> ChallengeResult:
    """æ¨¡å—çº§éªŒè¯ï¼ˆçº¯æ–‡æœ¬æ¨¡å¼ï¼‰ã€‚"""
    return _get_challenger().verify(subject, context)


async def achallenge(
    subject: str,
    context: str = "",
    *,
    max_tool_calls: int = 20,
) -> ChallengeResult:
    """æ¨¡å—çº§ MCP å·¥å…·å¢žå¼ºè´¨è¯¢ï¼ˆasyncï¼Œè‡ªåŠ¨è¿žæŽ¥ Serenaï¼‰ã€‚"""
    return await _get_challenger().challenge_with_tools(
        subject, context, max_tool_calls=max_tool_calls,
    )


async def averify(
    subject: str,
    context: str = "",
    *,
    max_tool_calls: int = 20,
) -> ChallengeResult:
    """æ¨¡å—çº§ MCP å·¥å…·å¢žå¼ºéªŒè¯ï¼ˆasyncï¼Œè‡ªåŠ¨è¿žæŽ¥ Serenaï¼‰ã€‚"""
    return await _get_challenger().verify_with_tools(
        subject, context, max_tool_calls=max_tool_calls,
    )


# â”€â”€ CLI å…¥å£ â”€â”€

if __name__ == "__main__":
    import argparse
    import sys

    from dotenv import load_dotenv

    load_dotenv()

    parser = argparse.ArgumentParser(description="Gemini è´¨è¯¢å·¥ä½ CLI")
    parser.add_argument(
        "mode",
        choices=["challenge", "verify"],
        help="è´¨è¯¢æ¨¡å¼",
    )
    parser.add_argument(
        "subject",
        help="è´¨è¯¢/éªŒè¯ç›®æ ‡",
    )
    parser.add_argument(
        "--context",
        default="",
        help="ä¸Šä¸‹æ–‡ä¿¡æ¯",
    )
    parser.add_argument(
        "--context-file",
        default=None,
        help="ä»Žæ–‡ä»¶è¯»å–ä¸Šä¸‹æ–‡",
    )
    parser.add_argument(
        "--tools",
        action="store_true",
        help="å¯ç”¨ MCP å·¥å…·æ¨¡å¼ï¼ˆGemini å¯è®¿é—® Serena è¯­ä¹‰å·¥å…·ï¼‰",
    )
    parser.add_argument(
        "--max-tool-calls",
        type=int,
        default=20,
        help="MCP æ¨¡å¼ä¸‹æœ€å¤§å·¥å…·è°ƒç”¨æ¬¡æ•°ï¼ˆé»˜è®¤ 20ï¼‰",
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="è¾“å‡ºå®Œæ•´æŽ¨ç†é“¾ï¼ˆGemini çš„æ¯æ­¥æ€è€ƒ + å·¥å…·è°ƒç”¨ + å·¥å…·è¿”å›žï¼‰",
    )
    args = parser.parse_args()

    ctx = args.context
    if args.context_file:
        with open(args.context_file, encoding="utf-8") as f:
            ctx = f.read()

    try:
        challenger = GeminiChallenger()
    except ValueError as e:
        print(f"Error: {e}", file=sys.stderr)
        sys.exit(1)

    if args.tools:
        # MCP å·¥å…·æ¨¡å¼ï¼ˆasyncï¼‰
        async def _run() -> ChallengeResult:
            if args.mode == "challenge":
                return await challenger.challenge_with_tools(
                    args.subject, ctx, max_tool_calls=args.max_tool_calls,
                )
            return await challenger.verify_with_tools(
                args.subject, ctx, max_tool_calls=args.max_tool_calls,
            )

        result = asyncio.run(_run())
    else:
        # çº¯æ–‡æœ¬æ¨¡å¼ï¼ˆsyncï¼‰
        if args.mode == "challenge":
            result = challenger.challenge(args.subject, ctx)
        else:
            result = challenger.verify(args.subject, ctx)

    print(f"[{result.mode}] model={result.model}")
    if result.tool_calls:
        print(f"tool_calls ({len(result.tool_calls)}):")
        for tc in result.tool_calls:
            print(f"  â†’ {tc}")
    if args.verbose and result.reasoning_chain:
        print()
        print("=== Gemini æŽ¨ç†é“¾ ===")
        for i, step in enumerate(result.reasoning_chain, 1):
            stype = step["type"]
            if stype == "thought":
                print(f"[{i}] ðŸ’­ {step['content']}")
            elif stype == "tool_call":
                args_str = ", ".join(
                    f"{k}={v!r}" for k, v in step.get("args", {}).items()
                )
                print(f"[{i}] ðŸ” {step['name']}({args_str})")
            elif stype == "tool_result":
                content = step.get("content", "")
                preview = content[:200] + "..." if len(content) > 200 else content
                print(f"    â†’ {step.get('name', '')}: {preview}")
        print()
    print("=" * 60)
    print(result.response)
